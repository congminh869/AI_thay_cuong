{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f2d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages, LoadWebcam\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "from utils.datasets import letterbox\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DETECT():\n",
    "    def __init__(self, model_total, device, half):\n",
    "        self.model_detect, self.stride_detect, self.names_detect = model_total['detect']\n",
    "        self.model_owner, self.stride_owner, self.names_owner = model_total['owner']\n",
    "        self.model_serial, self.stride_serial, self.names_serial = model_total['serial']\n",
    "        self.model_iso, self.stride_iso, self.names_iso = model_total['iso']\n",
    "        self.model_digit, self.stride_digit, self.names_digit = model_total['digit']\n",
    "        self.device = device\n",
    "        self.half = half\n",
    "        self.string_final = ''\n",
    "    def catchframe(self, frame):\n",
    "        if frame is None:\n",
    "            Print(\"Image is empty\")\n",
    "            return None\n",
    "\n",
    "        t2 = time.time()\n",
    "        frame, string_result = DETECT.detect_obj(self, self.model_detect, self.stride_detect, self.names_detect, img_detect =frame, iou_thres = 0.4, conf_thres = 0.5, img_size = 640)\n",
    "        t3 = time.time()\n",
    "        #print('++++++++++++++++++++++++Processing in %.3f'%(t3-t2))\n",
    "        return frame, string_result\n",
    "        \n",
    "    def rmsame(lst):\n",
    "        for inter in range(len(lst)):\n",
    "            check_local = []\n",
    "            for inter1 in range(len(lst)-inter-1):\n",
    "                if abs(lst[inter][0] - lst[inter1][0]) <2 and abs(lst[inter][1] - lst[inter1][1]) <2:\n",
    "                    if lst[inter][3]>lst[inter1][3]:\n",
    "                        lst.remove(lst[inter1])\n",
    "                    elif lst[inter][3]<lst[inter1][3]:\n",
    "                        lst.remove(lst[inter])\n",
    "        return lst\n",
    "    def detect_obj(self, model, stride, names , img_detect = '', iou_thres = 0.4, conf_thres = 0.5, img_size = 640):\n",
    "        imgsz = img_size\n",
    "        high, weight = img_detect.shape[:2]\n",
    "        #####################################\n",
    "        classify = False\n",
    "        agnostic_nms = False\n",
    "        augment = False\n",
    "        # Set Dataloader\n",
    "            #vid_path, vid_writer = None, None\n",
    "        # Get names and colors\n",
    "\n",
    "        # Run inference\n",
    "        t0 = time.time()\n",
    "        #processing images\n",
    "        '''\n",
    "        Tiá»n xá»­ lÃ­ áº£nh\n",
    "        '''\n",
    "        im0 = letterbox(img_detect, 640, stride= 32)[0]\n",
    "        im0 = im0[:, :, ::-1].transpose(2, 0, 1)\n",
    "        im0 = np.ascontiguousarray(im0)\n",
    "        im0 = torch.from_numpy(im0).to(self.device)\n",
    "        im0 = im0.half() if self.half else im0.float()\n",
    "        im0 /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if im0.ndimension() == 3:\n",
    "            im0 = im0.unsqueeze(0)\n",
    "        # Inference\n",
    "        t1 = time.time()\n",
    "        pred = model(im0, augment= False)[0]\n",
    "        # Apply NMS\n",
    "        classes = None\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes = classes, agnostic=agnostic_nms)\n",
    "    \n",
    "        # Apply Classifier\n",
    "        if classify:\n",
    "            pred = apply_classifier(pred, modelc, im0, img_ocr)\n",
    "        gn = torch.tensor(img_detect.shape)[[1, 0, 1, 0]]# normalization gain whwh\n",
    "        result = dict()\n",
    "        string = []\n",
    "        container = dict()\n",
    "        for key in names:\n",
    "            container[key] = ''\n",
    "            result[key] = '*'\n",
    "        if len(pred[0]):\n",
    "            pred[0][:, :4] = scale_coords(im0.shape[2:], pred[0][:, :4], img_detect.shape).round()\n",
    "            for c in pred[0][:, -1].unique():\n",
    "                n = (pred[0][:, -1] == c).sum()  # detections per class\n",
    "            count = 0\n",
    "            for box in pred[0]:\n",
    "                c1 = (int(box[0]), int(box[1]))\n",
    "                c2 = (int(box[2]), int(box[3]))\n",
    "                x1, y1 = c1\n",
    "                x2, y2 = c2\n",
    "                acc = round(float(box[4])*100,2)\n",
    "                cls = int(box[5])\n",
    "                label = names[cls]\n",
    "                img_crop = img_detect[y1:y2, x1:x2]\n",
    "                xyxy = [x1, y1, x2, y2]\n",
    "                image_detect = cv2.rectangle(img_detect, c1, c2, (255, 0, 0), 1)\n",
    "                result[label] = [xyxy, img_crop]\n",
    "            if container['owner'] != None and result['owner'] != '*':\n",
    "                container['owner'] = DETECT.detect_ocr(self, self.model_owner, self.stride_owner, 'owner', result['owner'][0], self.names_owner,\n",
    "                                                img_ocr = result['owner'][1], iou_thres = 0.4, conf_thres = 0.5, img_size = 640)\n",
    "            if container['serial'] != None and result['serial'] != '*':\n",
    "                container['serial'] = DETECT.detect_ocr(self, self.model_serial, self.stride_serial, 'serial', result['serial'][0], self.names_serial,\n",
    "                                                img_ocr = result['serial'][1], iou_thres = 0.4, conf_thres = 0.5, img_size = 640)\n",
    "            if container['ISO'] != None and result['ISO'] != '*':\n",
    "                container['ISO'] = DETECT.detect_ocr(self, self.model_iso, self.stride_iso, 'ISO', result['ISO'][0], self.names_iso,\n",
    "                                                img_ocr = result['ISO'][1], iou_thres = 0.4, conf_thres = 0.5, img_size = 640)\n",
    "            if container['check'] != None and result['check'] != '*':\n",
    "                container['check'] = DETECT.detect_ocr(self, self.model_digit, self.stride_digit, 'check', result['check'][0], self.names_digit,\n",
    "                                                img_ocr = result['check'][1], iou_thres = 0.4, conf_thres = 0.5, img_size = 640)\n",
    "            self.string_final = container['owner'] +' '+ container['serial'] +' '+ container['check'] +' '+ container['ISO'] \n",
    "            #image_detect = cv2.rectangle(img_detect, c1, c2, (255, 0, 0), 1)\n",
    "            #cv2.putText(image_detect, label, c1, cv2.FONT_HERSHEY_SIMPLEX,0.2, (255, 0, 0), 1)\n",
    "            #print(str(label)+'      '+''.join(string))\n",
    "            #print(string_result)\n",
    "            cv2.putText(img_detect, self.string_final , (0,high-3), cv2.FONT_HERSHEY_SIMPLEX,1, (255, 0, 0), 2)\n",
    "        return img_detect, self.string_final\n",
    "    def detect_ocr(self, model, stride, labels, xyxy, names1, img_ocr = '', iou_thres = 0.4, conf_thres = 0.5, img_size = 640):\n",
    "        h_1 = int(xyxy[3])-int(xyxy[1])\n",
    "        w_1 = int(xyxy[2])-int(xyxy[0])\n",
    "        labels_1 = labels\n",
    "        imgsz_1 = img_size\n",
    "        classify_1 = False\n",
    "        agnostic_nms_1 = False\n",
    "        augment_1 = False\n",
    "        names1_1 = names1\n",
    "        # Set Dataloader\n",
    "        #vid_path, vid_writer = None, None\n",
    "        # Get names and colors\n",
    "\n",
    "        # Run inference\n",
    "        t0_1 = time.time()\n",
    "        #processing images\n",
    "        '''\n",
    "        Tiá»n xá»­ lÃ­ áº£nh\n",
    "        '''\n",
    "        im0_1 = letterbox(img_ocr, 640, 32)[0]\n",
    "        im0_1 = im0_1[:, :, ::-1].transpose(2, 0, 1)\n",
    "        im0_1 = np.ascontiguousarray(im0_1)\n",
    "    \n",
    "        #####################################\n",
    "        im0_1 = torch.from_numpy(im0_1).to(self.device)\n",
    "        im0_1 = im0_1.half() if self.half else im0_1.float()\n",
    "        im0_1 /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if im0_1.ndimension() == 3:\n",
    "            im0_1 = im0_1.unsqueeze(0)\n",
    "        # Inference\n",
    "        t1_1 = time.time()\n",
    "        pred_1 = model(im0_1, augment= augment_1)[0]\n",
    "        # Apply NMS\n",
    "        classes = None\n",
    "        pred_1 = non_max_suppression(pred_1, conf_thres, iou_thres, classes = classes, agnostic=agnostic_nms_1)\n",
    "    \n",
    "        # Apply Classifier\n",
    "        if classify_1:\n",
    "            pred_1 = apply_classifier(pred_1, modelc, im0_1, img_ocr)\n",
    "        gn_1 = torch.tensor(img_ocr.shape)[[1, 0, 1, 0]]# normalization gain whwh\n",
    "        result_1 = []\n",
    "        string_1 = []\n",
    "        if len(pred_1[0]):\n",
    "            pred_1[0][:, :4] = scale_coords(im0_1.shape[2:], pred_1[0][:, :4], img_ocr.shape).round()\n",
    "            for c_1 in pred_1[0][:, -1].unique():\n",
    "                n_1 = (pred_1[0][:, -1] == c_1).sum()  # detections per class\n",
    "            count_1 = 0\n",
    "            for box_1 in pred_1[0]:\n",
    "                c1_1 = (int(box_1[0]), int(box_1[1]))\n",
    "                c2_1 = (int(box_1[2]), int(box_1[3]))\n",
    "                acc_1 = round(float(box_1[4])*100,2)\n",
    "                cls_1 = int(box_1[5])\n",
    "                label_1 = names1[cls_1]\n",
    "                #image_ocr = cv2.rectangle(img_ocr, c1, c2, (255, 0, 0), 1)\n",
    "                if acc_1 >0.7:\n",
    "                    result_1.append([int(box_1[0]), int(box_1[1]), label_1, acc_1])\n",
    "                count_1 += 1\n",
    "            result_1=DETECT.rmsame(result_1)\n",
    "            if h_1 < w_1 :\n",
    "                for m in range(len(result_1)):\n",
    "                    for n in range(len(result_1)-m-1):\n",
    "                        if result_1[m][0] > result_1[n+m+1][0]:\n",
    "                            middle = result_1[m]\n",
    "                            result_1[m] = result_1[n+m+1]\n",
    "                            result_1[n+m+1] = middle\n",
    "                [string_1.append(lb[2]) for lb in result_1]\n",
    "            else:\n",
    "                for m in range(len(result_1)):\n",
    "                    for n in range(len(result_1)-m-1):\n",
    "                        if result_1[m][1] > result_1[n+m+1][1]:\n",
    "                            middle = result_1[m]\n",
    "                            result_1[m] = result_1[n+m+1]\n",
    "                            result_1[n+m+1] = middle\n",
    "                [string_1.append(lb[2]) for lb in result_1]\n",
    "            t2_1 = time.time()\n",
    "            #print('OCR in %f s'%(t2_1-t1_1))\n",
    "            #print(str(labels_1)+'      '+''.join(string_1))\n",
    "        return ''.join(string_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62013865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91934e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 603a9be torch 1.8.1+cu102 CUDA:0 (GeForce RTX 2060, 5932.0625MB)\n",
      "\n",
      "Model Summary: 224 layers, 7062001 parameters, 0 gradients, 16.4 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 308 layers, 21138663 parameters, 0 gradients, 50.6 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 308 layers, 21074007 parameters, 0 gradients, 50.4 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 308 layers, 21078048 parameters, 0 gradients, 50.4 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 308 layers, 21074007 parameters, 0 gradients, 50.4 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Loaded model in 4.179751 s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'device' and 'half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8cd9fdd702f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mmodel_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'digit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded model in %f s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2_loadmodel\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1_loadmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mdetect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDETECT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatchframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'device' and 'half'"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     check_requirements(exclude=('pycocotools', 'thop'))\n",
    "#     #Khai bÃ¡o\n",
    "#     img_size = 640\n",
    "#     conf_thres = 0.25\n",
    "#     iou_thres = 0.45\n",
    "#     device = ''\n",
    "#     update = True\n",
    "#     model_total = dict()\n",
    "#     # Load model\n",
    "#     model_total =  dict()\n",
    "#     t1_loadmodel = time.time()\n",
    "#     set_logging()\n",
    "#     device = select_device(device)\n",
    "#     half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "#     # Load model nhan dien container\n",
    "#     weights = './weights/container.pt'\n",
    "#     model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "#     stride = int(model.stride.max())  # model stride\n",
    "#     names = model.module.names if hasattr(model, 'module') else model.names\n",
    "#     # Load model nhan dien owner\n",
    "#     weights1 = './weights/owner.pt'\n",
    "#     model1 = attempt_load(weights1, map_location=device)  # load FP32 model\n",
    "#     stride1 = int(model1.stride.max())  # model stride\n",
    "#     names1 = model1.module.names if hasattr(model1, 'module') else model1.names\n",
    "#     # Load model nhan dien serial\n",
    "#     weights2 = './weights/serial.pt'\n",
    "#     model2 = attempt_load(weights2, map_location=device)  # load FP32 model\n",
    "#     stride2 = int(model2.stride.max())  # model stride\n",
    "#     names2 = model2.module.names if hasattr(model2, 'module') else model2.names\n",
    "#     # Load model nhan dien ISO\n",
    "#     weights3 = './weights/iso.pt'\n",
    "#     model3 = attempt_load(weights3, map_location=device)  # load FP32 model\n",
    "#     stride3 = int(model3.stride.max())  # model stride\n",
    "#     names3 = model3.module.names if hasattr(model3, 'module') else model3.names\n",
    "#     # Load model nhan dien check\n",
    "#     weights4 = './weights/check.pt'\n",
    "#     model4 = attempt_load(weights4, map_location=device)  # load FP32 model\n",
    "#     stride4 = int(model4.stride.max())  # model stride\n",
    "#     names4 = model4.module.names if hasattr(model4, 'module') else model4.names\n",
    "#     if half:\n",
    "#         model.half()\n",
    "#         model1.half()\n",
    "#         model2.half()\n",
    "#         model3.half()\n",
    "#         model4.half()# to FP16\n",
    "#     t2_loadmodel = time.time()\n",
    "#     model_total['detect'] = (model, stride, names)\n",
    "#     model_total['owner'] = (model1, stride1, names1)\n",
    "#     model_total['serial'] = (model2, stride2, names2)\n",
    "#     model_total['iso'] = (model3, stride4, names3)\n",
    "#     model_total['digit'] = (model4, stride4, names4)\n",
    "#     print('Loaded model in %f s'%(t2_loadmodel-t1_loadmodel))\n",
    "#     detect = DETECT(model_total)\n",
    "#     img = detect.catchframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3ab037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages, LoadWebcam\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "from utils.datasets import letterbox\n",
    "import glob\n",
    "import os\n",
    "from capture import DETECT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb46ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "553cfd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2021-4-12 torch 1.8.1 CPU\n",
      "\n",
      "Model Summary: 224 layers, 7062001 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 308 layers, 21138663 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 308 layers, 21074007 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 308 layers, 21078048 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 308 layers, 21074007 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in 2.780909 s\n"
     ]
    }
   ],
   "source": [
    "#Khai bÃ¡o\n",
    "loai = '1'\n",
    "flag_all_done = 0\n",
    "list_container_codes = dict()\n",
    "img_size = 640\n",
    "conf_thres = 0.25\n",
    "iou_thres = 0.45\n",
    "device = ''\n",
    "update = True\n",
    "model_total = dict()\n",
    "# Load model\n",
    "model_total =  dict()\n",
    "t1_loadmodel = time.time()\n",
    "set_logging()\n",
    "device = select_device(device)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "# Load model nhan dien container\n",
    "weights = './weights/container.pt'\n",
    "model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "stride = int(model.stride.max())  # model stride\n",
    "names = model.module.names if hasattr(model, 'module') else model.names\n",
    "# Load model nhan dien owner\n",
    "weights1 = './weights/owner.pt'\n",
    "model1 = attempt_load(weights1, map_location=device)  # load FP32 model\n",
    "stride1 = int(model1.stride.max())  # model stride\n",
    "names1 = model1.module.names if hasattr(model1, 'module') else model1.names\n",
    "# Load model nhan dien serial\n",
    "weights2 = './weights/serial.pt'\n",
    "model2 = attempt_load(weights2, map_location=device)  # load FP32 model\n",
    "stride2 = int(model2.stride.max())  # model stride\n",
    "names2 = model2.module.names if hasattr(model2, 'module') else model2.names\n",
    "# Load model nhan dien ISO\n",
    "weights3 = './weights/iso.pt'\n",
    "model3 = attempt_load(weights3, map_location=device)  # load FP32 model\n",
    "stride3 = int(model3.stride.max())  # model stride\n",
    "names3 = model3.module.names if hasattr(model3, 'module') else model3.names\n",
    "# Load model nhan dien check\n",
    "weights4 = './weights/check.pt'\n",
    "model4 = attempt_load(weights4, map_location=device)  # load FP32 model\n",
    "stride4 = int(model4.stride.max())  # model stride\n",
    "names4 = model4.module.names if hasattr(model4, 'module') else model4.names\n",
    "if half:\n",
    "    model.half()\n",
    "    model1.half()\n",
    "    model2.half()\n",
    "    model3.half()\n",
    "    model4.half()# to FP16\n",
    "t2_loadmodel = time.time()\n",
    "model_total['detect'] = (model, stride, names)\n",
    "model_total['owner'] = (model1, stride1, names1)\n",
    "model_total['serial'] = (model2, stride2, names2)\n",
    "model_total['iso'] = (model3, stride4, names3)\n",
    "model_total['digit'] = (model4, stride4, names4)\n",
    "print('Loaded model in %f s'%(t2_loadmodel-t1_loadmodel))\n",
    "detect = DETECT(model_total, device, half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27dcdc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame is not None 1 0.021343469619750977\n",
      "high:  720\n",
      "weight 480\n",
      "im0 =  [[[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]]\n",
      "frame is not None 2 1.2033782005310059\n",
      "high:  720\n",
      "weight 480\n",
      "im0 =  [[[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]]\n",
      "frame is not None 3 2.400479316711426\n",
      "high:  720\n",
      "weight 480\n",
      "im0 =  [[[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]]\n",
      "frame is not None 4 3.7784008979797363\n",
      "high:  720\n",
      "weight 480\n",
      "im0 =  [[[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]]\n",
      "frame is not None 5 5.148923635482788\n",
      "high:  720\n",
      "weight 480\n",
      "im0 =  [[[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]\n",
      "\n",
      " [[114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [114 114 114]\n",
      "  [114 114 114]]]\n",
      "============Container2==================\n",
      "TRHU 102284 7 22G1\n",
      "*\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import cv2\n",
    "def ContainerMain(name='Container2',source=0, out='test5.jpg'):\n",
    "    global flag_all_done\n",
    "    i = 0\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    result = []\n",
    "    t0 = time.time()\n",
    "    while (time.time() - t0  < 6):\n",
    "        frame = cv2.imread('4.jpg')\n",
    "        if frame is not None:\n",
    "            i=i+1\n",
    "            print('frame is not None '+str(i)+ ' ' +str(time.time() - t0))\n",
    "            img2, string_result = detect.catchframe(frame)\n",
    "            cv2.imwrite(out, img2)\n",
    "            result.append(string_result)\n",
    "    if len(result)!=0:\n",
    "        dic = dict(Counter(result))\n",
    "        max_key = max(dic, key=dic.get)\n",
    "        print(\"============\" + name +\"==================\")\n",
    "        print(max_key)\n",
    "        flag_all_done = flag_all_done + 1\n",
    "        print('*')\n",
    "        id = int(name[-1])\n",
    "        print('**')\n",
    "        list_container_codes[id] = max_key\n",
    "        \n",
    "            \n",
    "ContainerMain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00278a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ContainerThread1\"\n",
    "from utils.datasets import letterboxname[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45584945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.datasets import letterbox\n",
    "frame = cv2.imread('4.jpg')\n",
    "im0 = letterbox(frame, 640, stride= 32)[0]\n",
    "type(im0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d996e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]]], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eea40ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 448, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54681955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 448, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im0 = im0[:, :, ::-1].transpose(2, 0, 1)\n",
    "im0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74800a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dbde816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 448, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im0 = np.ascontiguousarray(im0)\n",
    "im0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8de0eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]],\n",
       "\n",
       "       [[114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        ...,\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114],\n",
       "        [114, 114, 114]]], dtype=uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd27b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
